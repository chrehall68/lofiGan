{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lofi Gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wavfile\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "import keras.models as models\n",
    "import keras.losses as losses\n",
    "import keras.optimizers as optimizers\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"GPU\"\n",
    "DIRECTML_PLUGIN = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"C:/Users/Eliot/Documents/Audacity/lofi-part1.wav\"\n",
    "sample_rate, data = wavfile.read(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wave file of length 105.0 minutes and 44.35983333333297 seconds, which is 6344.359833333333 total seconds at sample rate 48000\n"
     ]
    }
   ],
   "source": [
    "total_seconds = len(data) / sample_rate\n",
    "minutes = total_seconds // 60\n",
    "seconds = total_seconds % 60\n",
    "print(\n",
    "    f\"wave file of length {minutes} minutes and {seconds} seconds, which is {total_seconds} total seconds at sample rate {sample_rate}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"max: {max(data)}, min: {min(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 32768.0, min: -32768.0\n",
      "max: 1.0, min: -1.0\n"
     ]
    }
   ],
   "source": [
    "# because we are using signed 16 bit PCM (ints), let's normalize our data to be between [-1, 1)\n",
    "float_data = np.array(data, dtype=np.float16)\n",
    "print(\n",
    "    f\"max: {np.max(float_data)}, min: {np.min(float_data)}\"\n",
    ")  # currently, still following int patterns of [-32768, +32767]\n",
    "float_data /= 32768\n",
    "print(\n",
    "    f\"max: {np.max(float_data)}, min: {np.min(float_data)}\"\n",
    ")  # now it falls between [-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SongSeparator:\\n    class Iterator:\\n        def __init__(self, ref) -> None:\\n            self.idx = 0\\n            self.ref = ref\\n            self.zeros = np.zeros((sample_rate,))\\n\\n        def __next__(self):\\n            if self.idx >= len(self.ref):\\n                raise StopIteration\\n\\n            if (\\n                self.idx < len(self.ref) - self.ref.sample_rate\\n                and (\\n                    self.ref.data[self.idx : self.idx + self.ref.sample_rate]\\n                    == self.zeros\\n                ).all()\\n            ):\\n                self.ref.num_songs += 1\\n\\n                times_between = 0\\n                while self.ref.data[i] == 0:\\n                    times_between += 1\\n                    self.idx += 1\\n                self.ref.max_times_between = max(\\n                    self.ref.max_times_between, times_between\\n                )\\n            else:\\n                self.idx += 1\\n\\n            return self.idx\\n\\n    def __init__(self, data: np.ndarray, sample_rate: int) -> None:\\n        self.length = len(data)\\n        self.num_songs = 0\\n        self.data = data\\n        self.sample_rate = sample_rate\\n        self.max_times_between = 0\\n\\n    def __len__(self):\\n        return self.length\\n\\n    def __iter__(self):\\n        return SongSeparator.Iterator(self)\\n\\na = SongSeparator(data, sample_rate)\\nfor i in tqdm(a):\\n  pass'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class SongSeparator:\n",
    "    class Iterator:\n",
    "        def __init__(self, ref) -> None:\n",
    "            self.idx = 0\n",
    "            self.ref = ref\n",
    "            self.zeros = np.zeros((sample_rate,))\n",
    "\n",
    "        def __next__(self):\n",
    "            if self.idx >= len(self.ref):\n",
    "                raise StopIteration\n",
    "\n",
    "            if (\n",
    "                self.idx < len(self.ref) - self.ref.sample_rate\n",
    "                and (\n",
    "                    self.ref.data[self.idx : self.idx + self.ref.sample_rate]\n",
    "                    == self.zeros\n",
    "                ).all()\n",
    "            ):\n",
    "                self.ref.num_songs += 1\n",
    "\n",
    "                times_between = 0\n",
    "                while self.ref.data[i] == 0:\n",
    "                    times_between += 1\n",
    "                    self.idx += 1\n",
    "                self.ref.max_times_between = max(\n",
    "                    self.ref.max_times_between, times_between\n",
    "                )\n",
    "            else:\n",
    "                self.idx += 1\n",
    "\n",
    "            return self.idx\n",
    "\n",
    "    def __init__(self, data: np.ndarray, sample_rate: int) -> None:\n",
    "        self.length = len(data)\n",
    "        self.num_songs = 0\n",
    "        self.data = data\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_times_between = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __iter__(self):\n",
    "        return SongSeparator.Iterator(self)\n",
    "\n",
    "a = SongSeparator(data, sample_rate)\n",
    "for i in tqdm(a):\n",
    "  pass\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "SHUFFLE_SIZE = 200\n",
    "SONG_LEN_IN_SECONDS = 15\n",
    "EPOCH_SIZE = 100\n",
    "START_SECTION = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "422"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs = []\n",
    "i = 0\n",
    "while i < len(float_data) - SONG_LEN_IN_SECONDS * sample_rate:\n",
    "    songs.append(float_data[i : i + SONG_LEN_IN_SECONDS * sample_rate])\n",
    "    i += SONG_LEN_IN_SECONDS * sample_rate\n",
    "\n",
    "songs = np.array(songs)\n",
    "len(songs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = songs[START_SECTION : START_SECTION + EPOCH_SIZE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    dataset = (\n",
    "        tf.data.Dataset.from_tensors(tf.constant(songs, dtype=tf.float16))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .shuffle(SHUFFLE_SIZE)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_INPUT_SIZE = (400, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input((SONG_LEN_IN_SECONDS * sample_rate,)))\n",
    "    model.add(layers.Reshape((SONG_LEN_IN_SECONDS * sample_rate, 1)))\n",
    "    if DIRECTML_PLUGIN:\n",
    "        model.add(layers.LSTM(2048, dropout=0.2, recurrent_dropout=0.2))\n",
    "    else:\n",
    "        model.add(layers.LSTM(2048))\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dense(512))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.ReLU())\n",
    "    model.add(layers.Dense(1))  # output neuron\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "with tf.device(DEVICE):\n",
    "    discriminator = make_discriminator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 720000, 1)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 2048)              16793600  \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1049088   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17,843,201\n",
      "Trainable params: 17,843,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(GENERATOR_INPUT_SIZE))\n",
    "    print(model.output_shape)\n",
    "\n",
    "    if DIRECTML_PLUGIN:\n",
    "        model.add(\n",
    "            layers.Bidirectional(\n",
    "                layers.LSTM(400 * 2, dropout=0.1, recurrent_dropout=0.1)\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        model.add(layers.Bidirectional(layers.LSTM(400 * 2)))\n",
    "    model.add(layers.Dense(400 * 16))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.ReLU())\n",
    "\n",
    "    model.add(layers.Reshape((400, 16)))\n",
    "\n",
    "    model.add(layers.Conv1DTranspose(14, 4, strides=8, padding=\"same\"))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.SyncBatchNormalization())\n",
    "    model.add(layers.ReLU())\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(\n",
    "        layers.Conv1DTranspose(\n",
    "            14 * SONG_LEN_IN_SECONDS, 4, strides=1, padding=\"same\", activation=\"tanh\"\n",
    "        )\n",
    "    )\n",
    "    print(model.output_shape)\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    print(model.output_shape)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 400, 2)\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "(None, 3200, 14)\n",
      "(None, 3200, 210)\n",
      "(None, 672000)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(DEVICE):\n",
    "    generator = make_generator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 1600)             5139200   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6400)              10246400  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6400)              0         \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 6400)              0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 400, 16)           0         \n",
      "                                                                 \n",
      " conv1d_transpose (Conv1DTra  (None, 3200, 14)         910       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 3200, 14)          0         \n",
      "                                                                 \n",
      " sync_batch_normalization (S  (None, 3200, 14)         56        \n",
      " yncBatchNormalization)                                          \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 3200, 14)          0         \n",
      "                                                                 \n",
      " conv1d_transpose_1 (Conv1DT  (None, 3200, 210)        11970     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 672000)            0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,398,536\n",
      "Trainable params: 15,398,508\n",
      "Non-trainable params: 28\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(DEVICE):\n",
    "    loss = losses.BinaryCrossentropy(from_logits=True)\n",
    "    generator_optimizer = optimizers.Adam(0.0001)\n",
    "    discriminator_optimizer = optimizers.Adadelta(0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "@tf.function\n",
    "def generator_loss(fake_pred):\n",
    "    return loss(tf.ones_like(fake_pred), fake_pred)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def discriminator_loss(true_pred, fake_pred):\n",
    "    true_loss = loss(tf.ones_like(true_pred), true_pred)\n",
    "    fake_loss = loss(tf.zeros_like(fake_pred), fake_pred)\n",
    "    return true_loss + fake_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 400, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a seed\n",
    "SAMPLES_TO_GENERATE = 8\n",
    "valid = False\n",
    "if os.path.exists(\"./seed.b\"):\n",
    "    with open(\"./seed.b\", \"rb\") as file:\n",
    "        seed = pickle.load(file)[\"seed\"]\n",
    "        if seed.shape == (SAMPLES_TO_GENERATE, *GENERATOR_INPUT_SIZE):\n",
    "            valid = True\n",
    "\n",
    "if not valid:\n",
    "    with open(\"./seed.b\", \"wb\") as file:\n",
    "        seed = tf.random.normal(\n",
    "            (SAMPLES_TO_GENERATE, *GENERATOR_INPUT_SIZE), dtype=tf.float16\n",
    "        )\n",
    "        pickle.dump({\"seed\": seed}, file)\n",
    "\n",
    "seed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_audio(gen, epoch_num, _seed):\n",
    "    inferences = generator(_seed, training=False)\n",
    "    inferences = inferences.numpy()\n",
    "    for i in range(len(inferences)):\n",
    "        wavfile.write(\n",
    "            f\"./generatedAudio/epoch_{epoch_num}_v{i}\", sample_rate, inferences[i]\n",
    "        )\n",
    "        display.Audio(inferences[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(audio):\n",
    "    noise = tf.random.normal([BATCH_SIZE, *GENERATOR_INPUT_SIZE])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_audio = generator(noise, training=True)\n",
    "\n",
    "        true_output = discriminator(audio, training=True)\n",
    "        fake_output = discriminator(generated_audio, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(true_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\n",
    "        disc_loss, discriminator.trainable_variables\n",
    "    )\n",
    "\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator, generator.trainable_variables)\n",
    "    )\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_discriminator, discriminator.trainable_variables)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    # get epoch\n",
    "    if os.path.exists(\"epoch.b\"):\n",
    "        with open(\"epoch.b\", \"rb\") as file:\n",
    "            epoch = pickle.load(file)[\"epoch\"]\n",
    "    else:\n",
    "        epoch = 0\n",
    "        with open(\"epoch.b\", \"wb\") as file:\n",
    "            pickle.dump({\"epoch\": epoch}, file)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "\n",
    "        # Produce images for the GIF as you go\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_audio(generator, epoch, seed)\n",
    "\n",
    "        # Save the model every 3 epochs\n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            generator.save(\"./generatorContinued\")\n",
    "            discriminator.save(\"./discriminatorContinued\")\n",
    "\n",
    "        print(\"Time for epoch {} is {} sec\".format(epoch + 1786, time.time() - start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_audio(generator, epochs, seed)\n",
    "\n",
    "    with open(\"epoch.b\", \"wb\") as file:\n",
    "        pickle.dump({\"epoch\": epoch}, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer \"lstm_cell_2\" (type LSTMCell).\n\nOOM when allocating tensor with shape[8,800] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator PluggableDevice_0_bfc [Op:MatMul]\n\nCall arguments received by layer \"lstm_cell_2\" (type LSTMCell):\n  • inputs=tf.Tensor(shape=(8, 2), dtype=float32)\n  • states=('tf.Tensor(shape=(8, 800), dtype=float32)', 'tf.Tensor(shape=(8, 800), dtype=float32)')\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Eliot\\Documents\\GitHub\\lofiGan\\lofiGan.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(DEVICE):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     train(dataset, \u001b[39m3\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Eliot\\Documents\\GitHub\\lofiGan\\lofiGan.ipynb Cell 33\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_batch \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     train_step(image_batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Produce images for the GIF as you go\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m display\u001b[39m.\u001b[39mclear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\Eliot\\Documents\\GitHub\\lofiGan\\lofiGan.ipynb Cell 33\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(audio)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m noise \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mnormal([BATCH_SIZE, \u001b[39m*\u001b[39mGENERATOR_INPUT_SIZE])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m gen_tape, tf\u001b[39m.\u001b[39mGradientTape() \u001b[39mas\u001b[39;00m disc_tape:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     generated_audio \u001b[39m=\u001b[39m generator(noise, training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     true_output \u001b[39m=\u001b[39m discriminator(audio, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Eliot/Documents/GitHub/lofiGan/lofiGan.ipynb#X51sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     fake_output \u001b[39m=\u001b[39m discriminator(generated_audio, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Eliot\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py:2223\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   2221\u001b[0m   out \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39msparse_dense_matmul(x, y)\n\u001b[0;32m   2222\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2223\u001b[0m   out \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mmatmul(x, y)\n\u001b[0;32m   2224\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer \"lstm_cell_2\" (type LSTMCell).\n\nOOM when allocating tensor with shape[8,800] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator PluggableDevice_0_bfc [Op:MatMul]\n\nCall arguments received by layer \"lstm_cell_2\" (type LSTMCell):\n  • inputs=tf.Tensor(shape=(8, 2), dtype=float32)\n  • states=('tf.Tensor(shape=(8, 800), dtype=float32)', 'tf.Tensor(shape=(8, 800), dtype=float32)')\n  • training=True"
     ]
    }
   ],
   "source": [
    "with tf.device(DEVICE):\n",
    "    train(dataset, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a569b528fb1110d0d7d552dfd5bf7c0920d164754c3ee6d9fc5930b2e92fc65e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
